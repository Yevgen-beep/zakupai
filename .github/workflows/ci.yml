name: ci

on:
  push: {}
  pull_request: {}

permissions:
  contents: read

env:
  # Stage6: legacy Week4 test suites are disabled by default; enable by
  # setting RUN_PRIORITY4_TESTS to "true" in workflow dispatch when needed.
  RUN_PRIORITY4_TESTS: "false"

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install linters
        run: |
          python -m pip install --upgrade pip
          pip install ruff black bandit

      - name: Ruff (lint)
        run: ruff check .

      - name: Black (format check)
        run: black --check .

      - name: Bandit (security check)
        run: |
          bandit -c .bandit \
            -x ".venv,venv,build,dist,migrations,__pycache__,node_modules,.git" \
            -r services/ libs/ bot/ web/ scripts/ tests/ *.py

  compose_sanity:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - name: Docker Compose config
        run: |
          [ -f bot/.env ] || cp bot/.env.example bot/.env
          docker compose config -q

  build:
    runs-on: ubuntu-latest
    needs: precommit
    concurrency:
      group: ci-build-${{ github.ref }}-${{ matrix.svc }}
      cancel-in-progress: true
    strategy:
      fail-fast: false
      matrix:
        svc: ["calc-service", "risk-engine", "doc-service", "embedding-api", "etl-service"]
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-qemu-action@v3
      - uses: docker/setup-buildx-action@v3
      - name: Build ${{ matrix.svc }}
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./services/${{ matrix.svc }}/Dockerfile
          push: false
          cache-from: type=gha
          cache-to: type=gha,mode=max
          provenance: false

  precommit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml', '**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-
      - name: Install pre-commit
        run: |
          python -m pip install --upgrade pip
          pip install pre-commit
      - name: Run pre-commit
        run: pre-commit run --all-files

  migrate:
    runs-on: ubuntu-latest
    needs: precommit
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - name: Start database
        run: |
          test -f bot/.env || cp bot/.env.example bot/.env
          docker compose up -d db
      - name: Ensure table ownership for migrator
        run: |
          [ -f .env ] || cp .env.example .env
          DB_SUPERUSER=$(grep -m1 '^POSTGRES_USER=' .env | cut -d'=' -f2-)
          DB_SUPERUSER=${DB_SUPERUSER:-postgres}
          DB_NAME=$(grep -m1 '^POSTGRES_DB=' .env | cut -d'=' -f2-)
          DB_NAME=${DB_NAME:-zakupai}

          tables=(subjects trdbuy lots contracts rnu)
          for table in "${tables[@]}"; do
            docker compose exec -T db psql -U "${DB_SUPERUSER}" -d "${DB_NAME}" \
              -c "ALTER TABLE IF EXISTS $table OWNER TO zakupai;" || true
          done
      - name: Apply bootstrap schema
        run: |
          [ -f .env ] || cp .env.example .env
          DB_SUPERUSER=$(grep -m1 '^POSTGRES_USER=' .env | cut -d'=' -f2-)
          DB_SUPERUSER=${DB_SUPERUSER:-postgres}
          DB_NAME=$(grep -m1 '^POSTGRES_DB=' .env | cut -d'=' -f2-)
          DB_NAME=${DB_NAME:-zakupai}

          test -f bot/.env || cp bot/.env.example bot/.env
          timeout 60 bash -c "until docker compose exec db pg_isready -U \"${DB_SUPERUSER}\" >/dev/null 2>&1; do sleep 2; done"
          for script in db/init/*.sql; do
            cat "$script" | docker compose exec -T db psql -U "${DB_SUPERUSER}" -d "${DB_NAME}" -v ON_ERROR_STOP=1 || exit 1
          done
      - name: Wait for database
        run: |
          test -f bot/.env || cp bot/.env.example bot/.env
          timeout 60 bash -c 'until docker compose exec db pg_isready -U zakupai; do sleep 2; done'
      - name: Run migrations
        run: |
          test -f bot/.env || cp bot/.env.example bot/.env
          docker compose run --rm migrator
      - name: Stop containers
        if: always()
        run: docker compose down

  pytest:
    runs-on: ubuntu-latest
    needs: migrate
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-
      - name: Install common lib
        run: pip install -e libs/zakupai_common
        if: github.event_name == 'push' || github.event_name == 'pull_request'
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: ${{ runner.os }}-pip-
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          find services -name requirements.txt -exec pip install -r {} \;
          pip install pytest pytest-timeout pytest-cov ruff black isort bandit
      - name: Run linters
        run: |
          ruff check .
          black --check .
          isort --check-only .
      - name: Run security checks
        run: |
          bandit -c .bandit \
            -x ".venv,venv,build,dist,migrations,__pycache__,node_modules,.git" \
            -r services/ libs/ bot/ web/ scripts/ tests/ *.py
      - name: Run tests
        run: pytest -q --cov=services --cov=libs --cov-report=term --disable-warnings

  deploy:
    runs-on: ubuntu-latest
    needs: [pytest, bandit-scan]
    if: github.ref == 'refs/heads/refactor/common-lib-test' && github.event_name == 'push'
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-buildx-action@v3

      - name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build & Push Docker images
        run: |
          docker build -t zakupai/gateway:stage5 -t zakupai/gateway:latest --build-arg TAG=stage5 ./services/gateway
          docker build -t zakupai/etl-service:stage5 -t zakupai/etl-service:latest --build-arg TAG=stage5 ./services/etl-service
          docker build -t zakupai/risk-engine:stage5 -t zakupai/risk-engine:latest --build-arg TAG=stage5 ./services/risk-engine
          docker build -t zakupai/calc-service:stage5 -t zakupai/calc-service:latest --build-arg TAG=stage5 ./services/calc-service
          docker build -t zakupai/embedding-api:stage5 -t zakupai/embedding-api:latest --build-arg TAG=stage5 ./services/embedding-api
          docker build -t zakupai/bot:stage5 -t zakupai/bot:latest --build-arg TAG=stage5 ./bot
          docker push zakupai/gateway:stage5
          docker push zakupai/gateway:latest
          docker push zakupai/etl-service:stage5
          docker push zakupai/etl-service:latest
          docker push zakupai/risk-engine:stage5
          docker push zakupai/risk-engine:latest
          docker push zakupai/calc-service:stage5
          docker push zakupai/calc-service:latest
          docker push zakupai/embedding-api:stage5
          docker push zakupai/embedding-api:latest
          docker push zakupai/bot:stage5
          docker push zakupai/bot:latest

      - name: Setup SSH key
        run: |
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ${{ runner.temp }}/ssh_key
          chmod 600 ${{ runner.temp }}/ssh_key

      - name: Deploy to staging
        run: |
          ssh -o ConnectTimeout=30 -o StrictHostKeyChecking=no -i ${{ runner.temp }}/ssh_key user@staging "cd /path/to/zakupai && export TAG=stage5 && docker compose -f docker-compose.yml -f docker-compose.override.prod.yml pull && docker compose -f docker-compose.yml -f docker-compose.override.prod.yml up -d --timeout 60"

      - name: Rollback on failure
        if: failure()
        run: |
          ssh -o ConnectTimeout=30 -o StrictHostKeyChecking=no -i ${{ runner.temp }}/ssh_key user@staging "cd /path/to/zakupai && export TAG=stable && docker compose -f docker-compose.yml -f docker-compose.override.prod.yml up -d --timeout 60"

  smoke-matrix:
    runs-on: ubuntu-latest
    needs: [migrate, build]
    strategy:
      matrix:
        service: [calc, risk, doc, emb]
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - name: Start db
        run: docker compose up -d db
      - name: Start ${{ matrix.service }} service
        run: |
          case "${{ matrix.service }}" in
            calc) docker compose up -d calc-service ;;
            risk) docker compose up -d risk-engine ;;
            doc) docker compose up -d doc-service ;;
            emb) docker compose up -d embedding-api ;;
          esac
      - name: Wait for service health
        run: |
          case "${{ matrix.service }}" in
            calc) port=8001 ;;
            risk) port=8002 ;;
            doc) port=8003 ;;
            emb) port=8004 ;;
          esac
          timeout 90 bash -c "until curl -fsS http://localhost:$port/health; do sleep 3; done"
      - name: Run smoke test
        run: bash scripts/smoke.sh

  etl-smoke-test:
    runs-on: ubuntu-latest
    needs: migrate
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-etl-${{ hashFiles('services/etl-service/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-etl-
      - name: Start database
        run: docker compose up -d db
      - name: Wait for database
        run: timeout 60 bash -c "until docker compose exec db pg_isready -U zakupai; do sleep 2; done"
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          cd services/etl-service
          pip install psycopg2-binary python-dotenv
      - name: Run ETL smoke test
        run: |
          cd services/etl-service
          chmod +x smoke_test.sh
          ./smoke_test.sh
        env:
          PGPASSWORD: zakupai
      - name: Upload smoke test logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: etl-smoke-test-logs
          path: |
            services/etl-service/smoke_test.log
            services/etl-service/ocr_loader.log
          retention-days: 7

  etl-upload-test:
    runs-on: ubuntu-latest
    needs: migrate
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-etl-${{ hashFiles('services/etl-service/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-etl-
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr tesseract-ocr-rus tesseract-ocr-eng
      - name: Start database
        run: docker compose up -d db
      - name: Wait for database
        run: timeout 60 bash -c "until docker compose exec db pg_isready -U zakupai; do sleep 2; done"
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          cd services/etl-service
          pip install -r requirements.txt
      - name: Run Alembic migrations
        run: |
          cd services/etl-service
          python -m alembic upgrade head
        env:
          DATABASE_URL: postgresql://zakupai:zakupai@localhost:5432/zakupai
      - name: Run ETL upload tests (pytest)
        run: |
          cd services/etl-service
          python -m pytest test_upload.py -v
        env:
          DATABASE_URL: postgresql://zakupai:zakupai@localhost:5432/zakupai
      - name: Build ETL service container
        run: |
          cd services/etl-service
          docker build -t zakupai-etl-service .
      - name: Run integration tests
        run: |
          cd services/etl-service
          bash test_etl_upload.sh
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: etl-upload-test-logs
          path: |
            services/etl-service/test_files/
          retention-days: 7

  priority3-integration:
    runs-on: ubuntu-latest
    needs: migrate
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-priority3-${{ hashFiles('services/etl-service/requirements.txt', 'services/embedding-api/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-priority3-
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr tesseract-ocr-rus tesseract-ocr-eng
      - name: Start required services
        run: docker compose up -d db chromadb embedding-api etl-service goszakup-api n8n
      - name: Wait for services to be ready
        run: |
          timeout 90 bash -c "until docker compose exec db pg_isready -U zakupai; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:8010/api/v2/heartbeat 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:7010/health 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:7011/health 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:7005/health 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:5678 2>/dev/null; do sleep 2; done"
      - name: Run ETL migrations
        run: make etl-migrate
        env:
          DATABASE_URL: postgresql://zakupai:zakupai@localhost:5432/zakupai
      - name: Run Priority 3 E2E tests
        run: make test-priority3
        env:
          DATABASE_URL: postgresql://zakupai:zakupai@localhost:5432/zakupai
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: priority3-test-logs
          path: |
            services/etl-service/test_files/
            services/etl-service/*.log
          retention-days: 7
      - name: Stop all containers
        if: always()
        run: docker compose down

  web-ui-integration:
    runs-on: ubuntu-latest
    needs: migrate
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-web-ui-${{ hashFiles('web/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-web-ui-
      - name: Start required services for web-ui
        run: docker compose up -d db etl-service
        env:
          GOSZAKUP_TOKEN: ${{ secrets.GOSZAKUP_TOKEN || 'test_token_for_ci' }}
          GOSZAKUP_BASE: https://ows.goszakup.gov.kz
      - name: Wait for services to be ready
        run: |
          timeout 60 bash -c "until docker compose exec db pg_isready -U zakupai; do sleep 2; done"
          timeout 60 bash -c "until curl -fsS http://localhost:7011/health 2>/dev/null; do sleep 2; done"
      - name: Build web-ui service
        run: |
          cd web
          docker build -t zakupai-web-ui .
      - name: Run web-ui service
        run: |
          docker run -d --name zakupai-web-ui-test \
            --network zakupai_zakupai-network \
            -p 8082:8000 \
            -e GOSZAKUP_TOKEN="${{ secrets.GOSZAKUP_TOKEN || 'test_token_for_ci' }}" \
            -e GOSZAKUP_BASE=https://ows.goszakup.gov.kz \
            -e ETL_SERVICE_URL=http://etl-service:8000 \
            zakupai-web-ui
      - name: Wait for web-ui to be ready
        run: |
          timeout 60 bash -c "until curl -fsS http://localhost:8082/health 2>/dev/null; do sleep 2; done"
      - name: Test web-ui endpoints
        run: |
          echo "Testing /health endpoint"
          curl -fsS http://localhost:8082/health | jq '.'

          echo "Testing /lots endpoint"
          response=$(curl -s -w '%{http_code}' http://localhost:8082/lots -o /tmp/lots_response.json)
          if [[ "$response" == "200" ]]; then
            echo "âœ… /lots endpoint returned HTTP 200"
            cat /tmp/lots_response.json | jq '.' | head -20
          elif [[ "$response" == "500" ]]; then
            echo "âš ï¸ /lots endpoint returned HTTP 500 (expected if GOSZAKUP_TOKEN is invalid)"
            cat /tmp/lots_response.json
          else
            echo "âŒ /lots endpoint returned HTTP $response"
            cat /tmp/lots_response.json
            exit 1
          fi
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: web-ui-test-logs
          path: |
            /tmp/lots_response.json
          retention-days: 7
      - name: Stop all containers
        if: always()
        run: |
          docker stop zakupai-web-ui-test || true
          docker rm zakupai-web-ui-test || true
          docker compose down

  webui-e2e-test:
    runs-on: ubuntu-latest
    needs: migrate
    steps:
      - uses: actions/checkout@v4
      - name: Create .env from example
        run: cp .env.example .env
      - name: Prepare bot .env
        run: cp bot/.env.example bot/.env
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-webui-${{ hashFiles('web/requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-webui-
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq tesseract-ocr tesseract-ocr-rus tesseract-ocr-eng
      - name: Start required services
        run: docker compose up -d db goszakup-api etl-service chromadb web-ui
        env:
          GOSZAKUP_TOKEN: test_token_for_ci
          GOSZAKUP_BASE: https://ows.goszakup.gov.kz
      - name: Wait for services to be ready
        run: |
          timeout 90 bash -c "until docker compose exec db pg_isready -U zakupai; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:8010/api/v2/heartbeat 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:7010/health 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:7011/health 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:7005/health 2>/dev/null; do sleep 2; done"
          timeout 90 bash -c "until curl -fsS http://localhost:8082/health 2>/dev/null; do sleep 2; done"
      - name: Check and create test fixture if missing
        run: |
          if [ ! -f web/test_fixtures/scan1.pdf ]; then
            echo "Creating dummy PDF fixture with Cyrillic content..."
            mkdir -p web/test_fixtures
            python3 -c "
            from reportlab.pdfgen import canvas
            import io
            pdf_buffer = io.BytesIO()
            c = canvas.Canvas(pdf_buffer)
            c.drawString(100, 750, 'Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ð´Ð»Ñ OCR')
            c.drawString(100, 720, 'Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾Ð± Ð¸ÑÐºÐµ Ð¾ Ð·Ð°ÐºÑƒÐ¿ÐºÐµ Ð»Ð°ÐºÐ°')
            c.drawString(100, 690, 'Ð”Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¿Ð¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¾Ð¼Ñƒ ÑÐ»Ð¾Ð²Ñƒ \"Ð¸ÑÐº\"')
            c.showPage()
            c.save()
            with open('web/test_fixtures/scan1.pdf', 'wb') as f:
                f.write(pdf_buffer.getvalue())
            "
          else
            echo "Test fixture scan1.pdf already exists"
          fi
        env:
          PYTHONPATH: .
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          cd web
          pip install -r requirements.txt
          pip install pytest pytest-timeout pytest-asyncio httpx reportlab
      - name: Run ETL migrations
        run: make etl-migrate
        env:
          DATABASE_URL: postgresql://zakupai:zakupai@localhost:5432/zakupai
      - name: Run Web UI E2E tests
        run: make webui-test
        env:
          DATABASE_URL: postgresql://zakupai:zakupai@localhost:5432/zakupai
      - name: Save test results
        if: always()
        run: |
          mkdir -p test_results
          if [ -f web/test_results.xml ]; then
            cp web/test_results.xml test_results/
          fi
          docker compose logs web-ui > test_results/webui.log 2>&1 || true
          docker compose logs etl-service > test_results/etl.log 2>&1 || true
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: webui-e2e-test-results
          path: |
            test_results/
            web/test_fixtures/
          retention-days: 7
      - name: Stop all containers
        if: always()
        run: docker compose down

  bandit-scan:
    runs-on: ubuntu-latest
    permissions:
      security-events: write
      contents: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install bandit
        run: pip install bandit[sarif]
      - name: Run bandit (SARIF)
        # Continue on error to not block pipeline, security findings visible in Security tab
        continue-on-error: true
        run: |
          bandit -c .bandit \
            -x ".venv,venv,build,dist,migrations,__pycache__,node_modules,.git" \
            -r services/ libs/ bot/ web/ scripts/ tests/ *.py \
            --severity-level medium \
            -f sarif -o bandit.sarif
      - name: Upload SARIF to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: bandit.sarif
          category: bandit-security-scan
      - name: Upload SARIF artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-sarif-report
          path: bandit.sarif
          retention-days: 30

  # Week 4.1: Integration tests for Web UI enhancements
  # Legacy Week4 integration suite (opt-in via RUN_PRIORITY4_TESTS)
  priority4-integration:
    if: ${{ env.RUN_PRIORITY4_TESTS == 'true' }}
    runs-on: ubuntu-latest
    continue-on-error: false
    env:
      DATABASE_URL: "postgresql://zakupai:test123@localhost:5432/zakupai_test"
    strategy:
      matrix:
        python-version: ["3.11", "3.12"]
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: zakupai_test
          POSTGRES_USER: zakupai
          POSTGRES_PASSWORD: test123
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      chromadb:
        image: ghcr.io/chroma-core/chroma:latest
        env:
          CHROMA_SERVER_HOST: 0.0.0.0
        ports:
          - 8000:8000

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ matrix.python-version }}-
            pip-${{ runner.os }}-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq postgresql-client redis-tools

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install fastapi uvicorn[standard] sqlalchemy psycopg2-binary jinja2
          pip install redis pandas chromadb httpx structlog pydantic apscheduler prometheus-fastapi-instrumentator
          pip install pytest pytest-timeout pytest-asyncio pytest-mock requests responses python-docx
          pip install python-telegram-bot aiohttp reportlab python-multipart

      - name: Set up test environment
        run: |
          # Wait for services to be ready
          sleep 10

          # Test database connection
          PGPASSWORD=test123 psql -h localhost -U zakupai -d zakupai_test -c "SELECT version();"

          # Test Redis connection
          redis-cli -h localhost ping

      - name: Run database migrations
        run: |
          # Apply all migrations including Week 4.1
          export DATABASE_URL="postgresql://zakupai:test123@localhost:5432/zakupai_test"

          # Run basic schema setup (mock)
          python -c "
          from sqlalchemy import create_engine, text
          import os

          engine = create_engine(os.getenv('DATABASE_URL'))
          with engine.connect() as conn:
              # Basic tables for testing
              conn.execute(text('''
                  CREATE TABLE IF NOT EXISTS lots (
                      id BIGSERIAL PRIMARY KEY,
                      nameRu TEXT,
                      amount NUMERIC(20,2),
                      lastUpdateDate TIMESTAMP,
                      trdBuyId BIGINT
                  );

                  CREATE TABLE IF NOT EXISTS trdbuy (
                      id BIGSERIAL PRIMARY KEY,
                      customerNameRu TEXT,
                      refBuyStatusId INT
                  );

                  CREATE TABLE IF NOT EXISTS prices (
                      id BIGSERIAL PRIMARY KEY,
                      product_name TEXT NOT NULL,
                      amount NUMERIC(20,2) NOT NULL CHECK (amount >= 0),
                      supplier_bin VARCHAR(12) NOT NULL,
                      imported_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                  );

                  CREATE TABLE IF NOT EXISTS import_logs (
                      id BIGSERIAL PRIMARY KEY,
                      file_name VARCHAR(255) NOT NULL,
                      status VARCHAR(20) NOT NULL,
                      total_rows INTEGER NOT NULL DEFAULT 0,
                      success_rows INTEGER NOT NULL DEFAULT 0,
                      error_rows INTEGER NOT NULL DEFAULT 0,
                      error_details JSONB,
                      processing_time_ms INTEGER,
                      imported_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                  );
              '''))
              conn.commit()
          print('Test database setup complete')
          "

      - name: Run Week 4.1 E2E Tests
        env:
          DATABASE_URL: "postgresql://zakupai:test123@localhost:5432/zakupai_test"
          REDIS_URL: "redis://localhost:6379/0"
          CHROMADB_URL: "http://localhost:8000"
          FLOWISE_API_URL: "http://mock-flowise:3000"
        run: |
          # Run specific Week 4.1 tests
          python -m pytest tests/test_e2e_week4.py -v --tb=short
          python -m pytest tests/test_import_prices.py -v --tb=short
          python -m pytest tests/test_lot_summary.py -v --tb=short

      - name: Start Web UI service for smoke tests
        env:
          DATABASE_URL: "postgresql://zakupai:test123@localhost:5432/zakupai_test"
          REDIS_URL: "redis://localhost:6379/0"
          CHROMADB_URL: "http://localhost:8000"
        run: |
          # Start web service in background
          cd web
          python main.py &
          WEB_PID=$!
          echo $WEB_PID > /tmp/web.pid

          # Wait for service to start
          sleep 5

          # Basic health check
          timeout 30 bash -c 'until curl -f http://localhost:8000/health 2>/dev/null; do sleep 1; done'

      - name: Run Smoke Tests
        env:
          BASE_URL: "http://localhost:8000"
          RISK_ENGINE_URL: "http://localhost:8001"  # Will fail gracefully
          TIMEOUT: "10"
        run: |
          # Run smoke tests
          ./scripts/smoke_tests.sh || echo "Some smoke tests failed (expected in CI)"

      - name: Cleanup
        if: always()
        run: |
          # Kill background processes
          if [ -f /tmp/web.pid ]; then
            kill $(cat /tmp/web.pid) 2>/dev/null || true
          fi

      - name: Test Results Summary
        if: always()
        run: |
          echo "Week 4.1 Integration Tests Summary:"
          echo "- Python version: ${{ matrix.python-version }}"
          echo "- Tests completed: $(date)"
          echo "- Check individual steps for detailed results"

  # Week 4.2: Performance testing with Locust
  # Legacy Week4 performance suite (opt-in via RUN_PRIORITY4_TESTS)
  priority4-performance:
    if: ${{ env.RUN_PRIORITY4_TESTS == 'true' }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11", "3.12"]
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: zakupai_test
          POSTGRES_USER: zakupai
          POSTGRES_PASSWORD: test123
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      flowise:
        image: flowiseai/flowise:latest
        env:
          FLOWISE_USERNAME: admin
          FLOWISE_PASSWORD: test123
        ports:
          - 3000:3000

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ matrix.python-version }}-performance-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ matrix.python-version }}-performance-
            pip-${{ runner.os }}-${{ matrix.python-version }}-
            pip-${{ runner.os }}-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq postgresql-client redis-tools

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install fastapi uvicorn[standard] sqlalchemy psycopg2-binary jinja2
          pip install redis pandas httpx structlog pydantic python-multipart apscheduler prometheus-fastapi-instrumentator
          pip install reportlab python-docx aiofiles
          pip install locust pytest pytest-timeout pytest-asyncio responses
          pip install prometheus-client

      - name: Set up test environment
        run: |
          # Wait for services to be ready
          sleep 15

          # Test database connection
          PGPASSWORD=test123 psql -h localhost -U zakupai -d zakupai_test -c "SELECT version();"

          # Test Redis connection
          redis-cli -h localhost ping

          # Test Flowise connection (with retries)
          timeout 60 bash -c 'until curl -f http://localhost:3000 2>/dev/null; do sleep 2; done' || echo "Flowise may not be ready, proceeding anyway"

      - name: Setup test database
        env:
          DATABASE_URL: "postgresql://zakupai:test123@localhost:5432/zakupai_test"
        run: |
          # Run all database migrations including Week 4.2
          python -c "
          from sqlalchemy import create_engine, text
          import os

          engine = create_engine(os.getenv('DATABASE_URL'))
          with engine.connect() as conn:
              # Apply Week 4.2 schema
              conn.execute(text('''
                  -- Basic tables
                  CREATE TABLE IF NOT EXISTS lots (
                      id BIGSERIAL PRIMARY KEY,
                      nameRu TEXT,
                      amount NUMERIC(20,2),
                      lastUpdateDate TIMESTAMP,
                      trdBuyId BIGINT
                  );

                  CREATE TABLE IF NOT EXISTS trdbuy (
                      id BIGSERIAL PRIMARY KEY,
                      customerNameRu TEXT,
                      refBuyStatusId INT
                  );

                  -- Week 4.2: Supplier sources
                  CREATE TABLE IF NOT EXISTS supplier_sources (
                      id SERIAL PRIMARY KEY,
                      name VARCHAR(50) NOT NULL UNIQUE,
                      url_template VARCHAR(255) NOT NULL,
                      parser_type VARCHAR(20) NOT NULL CHECK (parser_type IN ('api', 'web_search', 'mock')),
                      auth_type VARCHAR(10) NOT NULL CHECK (auth_type IN ('NONE', 'API_KEY', 'MCP')),
                      credentials_ref VARCHAR(100),
                      rate_limit INTEGER DEFAULT 100,
                      fallback_type VARCHAR(20) DEFAULT 'web_search',
                      active BOOLEAN DEFAULT true
                  );

                  -- Week 4.2: Complaints
                  CREATE TABLE IF NOT EXISTS complaints (
                      id BIGSERIAL PRIMARY KEY,
                      lot_id BIGINT NOT NULL,
                      complaint_text TEXT NOT NULL,
                      reason VARCHAR(255),
                      complaint_date DATE NOT NULL,
                      source VARCHAR(20) NOT NULL DEFAULT 'flowise',
                      format_type VARCHAR(10) DEFAULT 'text',
                      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                  );

                  -- Week 4.2: Suppliers cache
                  CREATE TABLE IF NOT EXISTS suppliers_cache (
                      id BIGSERIAL PRIMARY KEY,
                      cache_key VARCHAR(255) NOT NULL UNIQUE,
                      suppliers_data JSONB NOT NULL,
                      sources_used TEXT[],
                      total_results INTEGER DEFAULT 0,
                      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                      expires_at TIMESTAMP WITH TIME ZONE NOT NULL
                  );

                  -- Insert default supplier sources
                  INSERT INTO supplier_sources (name, url_template, parser_type, auth_type, rate_limit) VALUES
                  ('satu', 'https://satu.kz/search?q={query}', 'web_search', 'NONE', 60),
                  ('1688', 'https://s.1688.com/selloffer/offer_search.htm?keywords={query}', 'api', 'API_KEY', 100),
                  ('alibaba', 'https://www.alibaba.com/trade/search?SearchText={query}', 'api', 'API_KEY', 100)
                  ON CONFLICT (name) DO NOTHING;

                  -- Insert test lots
                  INSERT INTO lots (id, nameRu, amount, lastUpdateDate) VALUES
                  (12345, 'Ð¢ÐµÑÑ‚Ð¾Ð²Ð°Ñ Ð·Ð°ÐºÑƒÐ¿ÐºÐ° Ð¼ÐµÐ±ÐµÐ»Ð¸', 500000.00, NOW()),
                  (23456, 'ÐšÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð¾Ðµ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ', 750000.00, NOW()),
                  (34567, 'ÐšÐ°Ð½Ñ†ÐµÐ»ÑÑ€ÑÐºÐ¸Ðµ Ñ‚Ð¾Ð²Ð°Ñ€Ñ‹', 125000.00, NOW())
                  ON CONFLICT (id) DO NOTHING;
              '''))
              conn.commit()
          print('Week 4.2 test database setup complete')
          "

      - name: Start test server
        env:
          DATABASE_URL: "postgresql://zakupai:test123@localhost:5432/zakupai_test"
          REDIS_URL: "redis://localhost:6379/0"
          FLOWISE_API_URL: "http://localhost:3000"
        run: |
          # Start web service in background for performance testing
          cd web
          python main.py &
          WEB_PID=$!
          echo $WEB_PID > /tmp/web.pid

          # Wait for service to start
          sleep 10

          # Health check with retries
          timeout 60 bash -c 'until curl -f http://localhost:8000/health 2>/dev/null; do sleep 2; done'

          echo "Web service started successfully on port 8000"

      - name: Run Week 4.2 Unit Tests
        env:
          DATABASE_URL: "postgresql://zakupai:test123@localhost:5432/zakupai_test"
          REDIS_URL: "redis://localhost:6379/0"
          FLOWISE_API_URL: "http://localhost:3000"
        run: |
          # Run Week 4.2 feature tests
          python -m pytest tests/test_flowise_week4_2.py -v --tb=short

      - name: Run Advanced Search Tests
        env:
          BASE_URL: "http://localhost:8000"
        run: |
          # Run advanced search tests
          python -m pytest tests/test_advanced_search.py -v --tb=short

      - name: Run Locust Performance Tests
        env:
          BASE_URL: "http://localhost:8000"
          DATABASE_URL: "postgresql://zakupai:test123@localhost:5432/zakupai_test"
        run: |
          # Run Locust performance tests for 2 minutes
          cd tests

          echo "Starting Locust performance test..."
          locust -f locustfile.py \
            --host=http://localhost:8000 \
            --users=50 \
            --spawn-rate=5 \
            --run-time=2m \
            --headless \
            --html=locust_report.html \
            --csv=locust_stats

          echo "Performance test completed"

      - name: Validate Performance Targets
        run: |
          cd tests
          # Check if performance targets were met
          python -c "
          import csv
          import sys

          try:
              with open('locust_stats_stats.csv', 'r') as f:
                  reader = csv.DictReader(f)
                  stats = list(reader)

              targets_met = 0
              total_targets = 0

              print('Week 4.2 Performance Results:')
              print('=' * 50)

              for row in stats:
                  if row['Type'] == 'GET' or row['Type'] == 'POST':
                      name = row['Name']
                      avg_time = float(row['Average Response Time'])
                      p95_time = float(row['95%'])
                      failure_rate = float(row['Failure Count']) / (float(row['Request Count']) or 1) * 100

                      print(f'Endpoint: {name}')
                      print(f'  Avg response time: {avg_time:.0f}ms')
                      print(f'  95th percentile: {p95_time:.0f}ms')
                      print(f'  Failure rate: {failure_rate:.1f}%')

                      # Check specific targets
                      if 'complaint' in name.lower():
                          total_targets += 1
                          if p95_time < 1000:  # <1 second
                              targets_met += 1
                              print(f'  âœ… Complaint target met (<1s)')
                          else:
                              print(f'  âŒ Complaint target missed (>1s)')

                      elif 'supplier' in name.lower():
                          total_targets += 1
                          if p95_time < 1000:  # <1 second
                              targets_met += 1
                              print(f'  âœ… Supplier target met (<1s)')
                          else:
                              print(f'  âŒ Supplier target missed (>1s)')

                      elif 'autocomplete' in name.lower():
                          total_targets += 1
                          if p95_time < 500:  # <500ms
                              targets_met += 1
                              print(f'  âœ… Autocomplete target met (<500ms)')
                          else:
                              print(f'  âŒ Autocomplete target missed (>500ms)')

                      print()

              print(f'Performance Summary: {targets_met}/{total_targets} targets met')

              if targets_met == total_targets and total_targets > 0:
                  print('ðŸŽ‰ All performance targets achieved!')
                  sys.exit(0)
              elif targets_met >= total_targets * 0.8:
                  print('âš ï¸  Most performance targets met (80%+)')
                  sys.exit(0)
              else:
                  print('âŒ Performance targets not met')
                  sys.exit(1)

          except Exception as e:
              print(f'Error analyzing performance results: {e}')
              print('Performance validation skipped (test data may be incomplete)')
              sys.exit(0)
          "

      - name: Upload performance test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results-python-${{ matrix.python-version }}
          path: |
            tests/locust_report.html
            tests/locust_stats*.csv
          retention-days: 14

      - name: Cleanup
        if: always()
        run: |
          # Kill background processes
          if [ -f /tmp/web.pid ]; then
            kill $(cat /tmp/web.pid) 2>/dev/null || true
          fi

      - name: Performance Test Summary
        if: always()
        run: |
          echo "Week 4.2 Performance Test Summary:"
          echo "- Python version: ${{ matrix.python-version }}"
          echo "- Test duration: 2 minutes"
          echo "- Target users: 50 concurrent"
          echo "- Performance targets:"
          echo "  * Complaint generation: <1 second P95"
          echo "  * Supplier search: <1 second P95"
          echo "  * Autocomplete: <500ms P95"
          echo "- Results uploaded as artifacts"
          echo "- Tests completed: $(date)"
